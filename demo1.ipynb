{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efe48dac",
   "metadata": {},
   "source": [
    "# Digit Recognition\n",
    "\n",
    "![Digits 0 through 9](imgs/Digits_Cover.png)\n",
    "\n",
    "Run these blocks in order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f1bb1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yo. We're going to build and train this neural network right now.\n"
     ]
    }
   ],
   "source": [
    "from nn.network import NeuralNetwork\n",
    "from nn.layer import Layer\n",
    "from nn.loss import cross_entropy, cross_entropy_derivative # Best for classification yo\n",
    "from nn.activations import relu, relu_derivative, softmax, softmax_derivative_passthrough\n",
    "from nn.gd import fit\n",
    "from utils.loader import load_digits_dataset\n",
    "from utils.splitter import data_split\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print(\"Yo. We're going to build and train this neural network right now.\")\n",
    "\n",
    "np.random.seed(11)\n",
    "\n",
    "nn = NeuralNetwork(loss=cross_entropy, loss_derivative=cross_entropy_derivative)\n",
    "\n",
    "l2 = 0.001\n",
    "\n",
    "# 16x16 inputs = 256 features\n",
    "layer1 = Layer(input_size=256, output_size=128, activation=relu, d_activation=relu_derivative, mask=None, l2_lambda=l2)\n",
    "layer2 = Layer(input_size=128, output_size=64, activation=relu, d_activation=relu_derivative, mask=None, l2_lambda=l2)\n",
    "# Use softmax on output; derivative passthrough so CE gradient isnâ€™t multiplied again\n",
    "layer3 = Layer(input_size=64, output_size=10, activation=softmax, d_activation=softmax_derivative_passthrough, mask=None)\n",
    "\n",
    "nn.add_layers([layer1, layer2, layer3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03c5ebea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now we're loading the dataset...\n",
      "In Jupyter notebooks, it'll be pretty slow, but it shouldn't take more than 30 seconds (still a pretty big dataset to be fair).\n"
     ]
    }
   ],
   "source": [
    "print(\"Now we're loading the dataset...\")\n",
    "\n",
    "print(\"In Jupyter notebooks, it'll be pretty slow, but it shouldn't take more than 30 seconds (still a pretty big dataset to be fair).\")\n",
    "\n",
    "X, Y = load_digits_dataset()  # normalized with scale255\n",
    "\n",
    "X_train, Y_train, X_val, Y_val, X_test, Y_test = data_split(X, Y, train_ratio=0.8, val_ratio=0.1, seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c09ec42c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Done training.\n",
      "Final Test Error: 0.2531\n",
      "Train Image 1: Predicted Label = 4, Actual Label = 4\n",
      "Train Image 2: Predicted Label = 8, Actual Label = 8\n",
      "Train Image 3: Predicted Label = 7, Actual Label = 7\n",
      "Train Image 6: Predicted Label = 2, Actual Label = 2\n",
      "Train Image 11: Predicted Label = 4, Actual Label = 4\n",
      "Train Image 22: Predicted Label = 3, Actual Label = 1\n",
      "Train Image 33: Predicted Label = 8, Actual Label = 8\n",
      "Train Image 66: Predicted Label = 7, Actual Label = 7\n",
      "Train Image 137: Predicted Label = 3, Actual Label = 3\n",
      "Train Image 274: Predicted Label = 8, Actual Label = 8\n",
      "Train Image 411: Predicted Label = 4, Actual Label = 4\n",
      "Train Image 822: Predicted Label = 2, Actual Label = 2\n",
      "Train Image 1507: Predicted Label = 2, Actual Label = 2\n",
      "Train Image 3014: Predicted Label = 0, Actual Label = 0\n",
      "Train Image 4521: Predicted Label = 3, Actual Label = 3\n",
      "Train Image 9042: Predicted Label = 4, Actual Label = 4\n",
      "Train Accuracy: 93.90%\n"
     ]
    }
   ],
   "source": [
    "print(\"Training...\")\n",
    "# In demos.ipynb\n",
    "test_error = fit(nn, X_train, Y_train, X_val, Y_val, X_test, Y_test, max_epochs=200, learning_rate=0.005)\n",
    "\n",
    "print(\"Done training.\")\n",
    "print(f\"Final Test Error: {test_error:.4f}\")\n",
    "\n",
    "nn.save_model(\"digits_nn_model.pkl\")\n",
    "\n",
    "# Re-test on the actual training data itself to check for overfitting\n",
    "train_correct = 0\n",
    "train_total = len(X_train)\n",
    "for i in range(train_total):\n",
    "    output = nn.forward(X_train[i])\n",
    "    predicted_label = np.argmax(output, axis=1)[0]\n",
    "    actual_label = np.argmax(Y_train[i])\n",
    "\n",
    "    if train_total % (i + 1) == 0:\n",
    "        print(f\"Train Image {i + 1}: Predicted Label = {predicted_label}, Actual Label = {actual_label}\")\n",
    "\n",
    "    if predicted_label == actual_label:\n",
    "        train_correct += 1\n",
    "\n",
    "train_accuracy = (train_correct / train_total * 100) if train_total > 0 else 0.0\n",
    "print(f\"Train Accuracy: {train_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38bcfc0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now let's actually put this to use. In the data/digits/ folder, I have made a test/ folder with handwritten digits :)\n",
      "Test Image 1: Predicted Label = 0, Actual Label = 0\n",
      "Test Image 2: Predicted Label = 0, Actual Label = 0\n",
      "Test Image 3: Predicted Label = 0, Actual Label = 0\n",
      "Test Image 6: Predicted Label = 1, Actual Label = 1\n",
      "Test Image 11: Predicted Label = 2, Actual Label = 2\n",
      "Test Image 22: Predicted Label = 8, Actual Label = 5\n",
      "Test Image 33: Predicted Label = 8, Actual Label = 8\n",
      "Test Accuracy: 60.00%\n",
      "See, this is why we need convolutional neural networks: since Dense networks expect flat 1D inputs .\n"
     ]
    }
   ],
   "source": [
    "print(\"Now let's actually put this to use. In the data/digits/ folder, I have made a test/ folder with handwritten digits :)\")\n",
    "\n",
    "# Real test set\n",
    "X, Y = load_digits_dataset(\"data/digits/tests\", target_size=(16,16))\n",
    "correct = 0\n",
    "total = len(X)\n",
    "for i in range(total):\n",
    "    output = nn.forward(X[i])\n",
    "    predicted_label = np.argmax(output, axis=1)[0]\n",
    "    actual_label = np.argmax(Y[i])\n",
    "    if train_total % (i+1) == 0:\n",
    "        print(f\"Test Image {i + 1}: Predicted Label = {predicted_label}, Actual Label = {actual_label}\")\n",
    "\n",
    "    if predicted_label == actual_label:\n",
    "        correct += 1\n",
    "\n",
    "accuracy = (correct / total * 100) if total > 0 else 0.0 # Avoid division by zero although it should never happen\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "print(\"See, this is why we need convolutional neural networks: since Dense networks expect flat 1D inputs .\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cedc9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feel free to add more blocks and test your own handwritten digits. Rescale to 16x16 if needed and follow the same template as in here.\n",
      "You can reuse the model by loading it with NeuralNetwork.load_model(filename).\n",
      "They are saved as .pkl and HF5 files in the models/ folder.\n",
      "Note that, for this specific dataset, accuracy is going to be very low since we actually need a CNN. But the Dense network with regularization is still doing a decent job.\n"
     ]
    }
   ],
   "source": [
    "print(\"Feel free to add more blocks and test your own handwritten digits. Rescale to 16x16 if needed and follow the same template as in here.\")\n",
    "print(\"You can reuse the model by loading it with NeuralNetwork.load_model(filename).\")\n",
    "print(\"They are saved as .pkl and HF5 files in the models/ folder.\")\n",
    "print(\"Note that, for this specific dataset, accuracy is going to be very low since we actually need a CNN. But the Dense network with regularization is still doing a decent job.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
